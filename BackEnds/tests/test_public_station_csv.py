#!/usr/bin/env python3
"""Test public station CSV persistence and rebuild functionality.

Verifies that:
1. CSV files are written during fetch_weather.py execution
2. build_dataset.py correctly rebuilds public_stations table from CSVs
3. CSV cleanup removes files older than 30 days
"""

import csv
import glob
import os
import sqlite3
import sys

# Add the-snake-tank directory to path to import build_dataset functions
test_dir = os.path.dirname(os.path.abspath(__file__))
snake_tank_dir = os.path.join(os.path.dirname(test_dir), 'the-snake-tank')
sys.path.insert(0, snake_tank_dir)

# Change to the-snake-tank directory for file access
os.chdir(snake_tank_dir)

from build_dataset import _num, _int


def test_csv_format():
    """Verify CSV files have correct header and format."""
    csv_dir = os.path.join('data', 'public-stations')
    csvs = glob.glob(os.path.join(csv_dir, '*', '*.csv'))

    if not csvs:
        print("SKIP: No CSV files found (workflow hasn't run yet)")
        return

    print(f"Found {len(csvs)} CSV files")

    # Check the latest CSV
    latest = sorted(csvs)[-1]
    print(f"Checking: {latest}")

    with open(latest) as f:
        reader = csv.DictReader(f)
        header = reader.fieldnames

        expected_columns = [
            'fetched_at', 'station_id', 'lat', 'lon', 'temperature',
            'humidity', 'pressure', 'rain_60min', 'rain_24h',
            'wind_strength', 'wind_angle', 'gust_strength', 'gust_angle'
        ]

        assert header == expected_columns, f"Header mismatch: {header}"

        # Read at least one row to verify format
        rows = list(reader)
        assert len(rows) > 0, "CSV should contain data rows"

        # Verify fetched_at format (ISO timestamp)
        first_row = rows[0]
        assert 'T' in first_row['fetched_at'], "fetched_at should be ISO timestamp"

        print(f"  Header: {header}")
        print(f"  Rows: {len(rows)}")
        print("  Format: OK")


def test_database_schema():
    """Verify public_stations table has correct schema."""
    conn = sqlite3.connect('data/weather.db')

    # Check table exists
    tables = conn.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='public_stations'"
    ).fetchall()
    assert len(tables) == 1, "public_stations table should exist"

    # Check column count and names
    cols = conn.execute('PRAGMA table_info(public_stations)').fetchall()
    assert len(cols) == 14, f"Expected 14 columns, got {len(cols)}"

    col_names = [col[1] for col in cols]
    expected_cols = [
        'id', 'fetched_at', 'station_id', 'lat', 'lon', 'temperature',
        'humidity', 'pressure', 'rain_60min', 'rain_24h',
        'wind_strength', 'wind_angle', 'gust_strength', 'gust_angle'
    ]
    assert col_names == expected_cols, f"Column mismatch: {col_names}"

    # Check index exists
    indexes = conn.execute(
        "SELECT name FROM sqlite_master WHERE type='index' AND tbl_name='public_stations'"
    ).fetchall()
    index_names = [idx[0] for idx in indexes]
    assert 'idx_public_stations_time' in index_names, "Missing index on fetched_at"

    conn.close()
    print(f"  Columns: {len(cols)}")
    print(f"  Index: idx_public_stations_time exists")
    print("  Schema: OK")


def test_csv_rebuild():
    """Verify that rebuild populates table with correct data."""
    csv_dir = os.path.join('data', 'public-stations')
    csvs = glob.glob(os.path.join(csv_dir, '*', '*.csv'))

    if not csvs:
        print("SKIP: No CSV files to test rebuild")
        return

    conn = sqlite3.connect('data/weather.db')
    count = conn.execute('SELECT COUNT(*) FROM public_stations').fetchone()[0]

    print(f"  CSV files: {len(csvs)}")
    print(f"  DB rows: {count}")

    if count == 0:
        print("  WARNING: No rows in database â€” rebuild may not have run")
    else:
        # Verify at least one row has valid data
        sample = conn.execute(
            'SELECT fetched_at, station_id, lat, lon FROM public_stations LIMIT 1'
        ).fetchone()

        assert sample is not None, "Should have at least one row"
        assert sample[0] is not None, "fetched_at should not be NULL"
        assert sample[1] is not None, "station_id should not be NULL"

        print(f"  Sample: fetched_at={sample[0]}, station_id={sample[1]}")
        print("  Rebuild: OK")

    conn.close()


def test_type_conversion_helpers():
    """Verify _num and _int helper functions work correctly."""
    # Test _num (float conversion)
    assert _num(None) is None
    assert _num("") is None
    assert _num("3.14") == 3.14
    assert _num("42") == 42.0

    # Test _int (integer conversion)
    assert _int(None) is None
    assert _int("") is None
    assert _int("42") == 42
    assert _int("3.14") == 3  # Should truncate

    print("  Type conversion: OK")


if __name__ == "__main__":
    print("Testing public station CSV functionality...\n")

    print("Test 1: CSV format")
    test_csv_format()

    print("\nTest 2: Database schema")
    test_database_schema()

    print("\nTest 3: CSV rebuild")
    test_csv_rebuild()

    print("\nTest 4: Type conversion helpers")
    test_type_conversion_helpers()

    print("\nAll tests passed!")
